{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    \n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    #print(A.shape, A)\n",
    "    #print(W.shape, W)\n",
    "    Z = np.dot(W, A) + b\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    #print(\"cache\", cache)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    #print(\"cache\", cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    cost = - (np.dot(Y,np.log(AL.T)) + np.dot(1 - Y, np.log(1 - AL.T)))* (1.0/ m)\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dW = np.dot(dZ,A_prev.T)/m\n",
    "    db = np.sum(dZ, axis=1, keepdims=True)/m\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\" + str(l+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Forward propagation\n",
    "    A1, cache1 = linear_activation_forward(X, W1, b1, activation=\"relu\")\n",
    "    A2, cache2 = linear_activation_forward(A1, W2, b2, activation=\"sigmoid\")\n",
    "    \n",
    "    probas = A2\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, activation=\"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, activation=\"sigmoid\")\n",
    "        \n",
    "        # Compute cost\n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation=\"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation=\"relu\")\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate = learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 105\n",
      "Number of testing examples: m_test = 45\n",
      "number of features of each example: num_n = 4\n",
      "shape of x_train: (105, 4)\n",
      "shape of y_train: (105, 1)\n",
      "shape of x_test: (45, 4)\n",
      "shape of y_test: (45, 1)\n"
     ]
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "x = iris.data\n",
    "y = ((iris.target != 0) * 1)  # convert the target's dataset into binary\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3)\n",
    "\n",
    "m_train = len(x_train)\n",
    "m_test = len(x_test)\n",
    "num_n = x_train.shape[1]\n",
    "numtest_n = x_test.shape[1]\n",
    "\n",
    "y_train = y_train.reshape(m_train,1)\n",
    "y_test = y_test.reshape(m_test,1)\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"number of features of each example: num_n = \" + str(num_n))\n",
    "print(\"shape of x_train:\",x_train.shape)\n",
    "print(\"shape of y_train:\",y_train.shape)\n",
    "print(\"shape of x_test:\",x_test.shape)\n",
    "print(\"shape of y_test:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = x_train.shape[1]     # number of features\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6932004328386008\n",
      "Cost after iteration 100: 0.6778396908187353\n",
      "Cost after iteration 200: 0.6560189045836289\n",
      "Cost after iteration 300: 0.6031757228065021\n",
      "Cost after iteration 400: 0.5454872903345053\n",
      "Cost after iteration 500: 0.4703414208895231\n",
      "Cost after iteration 600: 0.3760606012054062\n",
      "Cost after iteration 700: 0.31558279806910916\n",
      "Cost after iteration 800: 0.27012247795134653\n",
      "Cost after iteration 900: 0.22387699177145542\n",
      "Cost after iteration 1000: 0.17177117718940665\n",
      "Cost after iteration 1100: 0.12589076775796598\n",
      "Cost after iteration 1200: 0.09381285087502125\n",
      "Cost after iteration 1300: 0.07239393531226077\n",
      "Cost after iteration 1400: 0.057685638912878395\n",
      "Cost after iteration 1500: 0.04727458242600048\n",
      "Cost after iteration 1600: 0.03964241809624494\n",
      "Cost after iteration 1700: 0.03387743824926098\n",
      "Cost after iteration 1800: 0.029411791992694714\n",
      "Cost after iteration 1900: 0.025876082459766567\n",
      "Cost after iteration 2000: 0.023022749904659592\n",
      "Cost after iteration 2100: 0.020681523683409135\n",
      "Cost after iteration 2200: 0.0187324151666726\n",
      "Cost after iteration 2300: 0.017088966611375028\n",
      "Cost after iteration 2400: 0.015687609621970258\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = two_layer_model(x_train.T, y_train.T, layers_dims = layers_dims, num_iterations = 2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(x_train.T, y_train.T, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(x_test.T, y_test.T, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
